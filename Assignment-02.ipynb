{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we are importing our libraries that we will use during our neural networks training (note that pandas will be used to turn the result to a csv file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Georgio\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from tensorflow.python.keras.utils import np_utils\n",
    "from keras.optimizers import SGD\n",
    "from keras.datasets import mnist\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "np.random.seed(42)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we are setting some constants that we will use later in our training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DEFAULT_BATCH_SIZE = 32\n",
    "DEFAULT_NUMBER_HIDDEN = 2\n",
    "DEFAULT_LEARNING_RATE = 0.01\n",
    "NUM_CLASSES = 10  \n",
    "EPOCHS = 5\n",
    "results = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we are using the mnist dataset (minst is a dataset of 28x28 images of handwritten digits and their labels\n",
    ") which we are loading from keras and then splitting it into train and test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_1, train_labels_1), (X_test, test_labels_1) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Reshapes `X_train_1` and `X_test` to have 784 columns each, converting 2D image data into 1D vectors for model input.\n",
    "- Converts data types to `float32` for both datasets to ensure compatibility with machine learning frameworks and optimize memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1 = X_train_1.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "X_train_1 = X_train_1.astype('float32')\n",
    "X_test = X_test.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizes the training and testing datasets to a range of 0 to 1 by dividing by 255, improving model training efficiency by ensuring inputs are on a consistent scale.\n",
    "\n",
    "its divided bu 255 because image data typically comes in the form of integers ranging from 0 to 255, representing the intensity of pixels in grayscale images (or each color channel in RGB images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1 /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splits the preprocessed dataset into a training set with the first 50,000 samples and a validation set with the last 10,000 samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 train samples\n",
      "10000 validation samples\n"
     ]
    }
   ],
   "source": [
    "# Divides the dataset into train and validation sets\n",
    "X_valid = X_train_1[50000:60000]\n",
    "X_train = X_train_1[:50000]\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_valid.shape[0], 'validation samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The label arrays are converted into a format that's more suitable for the model using the `np_utils.to_categorical` function. This process changes the training and testing labels into one-hot encoded vectors, determined by the total number of classes (`NUM_CLASSES`). In one-hot encoding, for each label, an array is created where all elements are 0 except for the position corresponding to the label, which is set to 1.\n",
    "\n",
    "Following this transformation for both training and testing labels, a subset of the training labels is allocated for validation purposes (`valid_labels`), extracted from the last 10,000 entries of the one-hot encoded training labels. Consequently, the `train_labels` array is adjusted to contain only the first 50,000 entries to align with the dataset's split for training, validation, and testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np_utils.to_categorical(train_labels_1, NUM_CLASSES)\n",
    "test_labels = np_utils.to_categorical(test_labels_1, NUM_CLASSES)\n",
    "valid_labels = train_labels[50000:60000]\n",
    "train_labels = train_labels[:50000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `create_model` is designed to easily experiment with different architectures by adjusting the number of hidden layers, input shape, and number of output classes. It allows for quick adjustments and testing of various configurations, facilitating the exploration of multiple combinations of hidden layers, batch sizes, and learning rates in the model training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_hidden_layers, input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, activation='relu', input_shape=(input_shape,)))\n",
    "    for _ in range(num_hidden_layers - 1):\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we are conducting a series of experiments to explore how different hyperparameters affect model performance. It iterates over sets of batch sizes, numbers of hidden layers, and learning rates, using a predefined function `create_model` to construct models with these varying configurations. For each configuration, the model is compiled and trained on a training set, then evaluated on a test set. The results, including hyperparameter values and model performance (accuracy and loss), are collected in a list and then converted into a DataFrame. This DataFrame is saved to a CSV file, `hyperparameter_tuning_results.csv`, allowing for easy analysis of how each hyperparameter impacts the model's effectiveness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_num in [1, 2, 4, 8, 16, 32, 64, 128]:\n",
    "    model = create_model(DEFAULT_NUMBER_HIDDEN, 784, NUM_CLASSES)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=SGD(learning_rate=DEFAULT_LEARNING_RATE), metrics=['accuracy'])\n",
    "    model.fit(X_train, train_labels, epochs=EPOCHS, batch_size=batch_num, verbose=1, validation_data=(X_valid, valid_labels))\n",
    "    score = model.evaluate(X_test, test_labels, batch_size=batch_num, verbose=1)\n",
    "\n",
    "    results.append({'Hyperparameter': 'BatchSize', \n",
    "                    'Value': batch_num, \n",
    "                    'Default_BatchSize': DEFAULT_BATCH_SIZE,\n",
    "                    'Default_NumHiddenLayers': DEFAULT_NUMBER_HIDDEN, \n",
    "                    'Default_LearningRate': DEFAULT_LEARNING_RATE, \n",
    "                    'Accuracy': score[1], \n",
    "                    'Loss': score[0]}) \n",
    "\n",
    "for hidden_num in [1, 2, 4, 6, 8]:\n",
    "    model = create_model(hidden_num, 784, NUM_CLASSES)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=SGD(learning_rate=DEFAULT_LEARNING_RATE), metrics=['accuracy'])\n",
    "    model.fit(X_train, train_labels, epochs=EPOCHS, batch_size=DEFAULT_BATCH_SIZE, verbose=1, validation_data=(X_valid, valid_labels))\n",
    "    score = model.evaluate(X_test, test_labels, batch_size=DEFAULT_BATCH_SIZE, verbose=1)\n",
    "    results.append({'Hyperparameter': 'NumHiddenLayers', \n",
    "                    'Value': hidden_num, \n",
    "                    'Default_BatchSize': DEFAULT_BATCH_SIZE,\n",
    "                    'Default_NumHiddenLayers': DEFAULT_NUMBER_HIDDEN, \n",
    "                    'Default_LearningRate': DEFAULT_LEARNING_RATE, \n",
    "                    'Accuracy': score[1], \n",
    "                    'Loss': score[0]})\n",
    "\n",
    "for learning_r in [0.01, 0.05, 0.1, 0.2, 0.4, 0.8]:\n",
    "    model = create_model(DEFAULT_NUMBER_HIDDEN, 784, NUM_CLASSES)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=SGD(learning_rate=learning_r), metrics=['accuracy'])\n",
    "    model.fit(X_train, train_labels, epochs=EPOCHS, batch_size=DEFAULT_BATCH_SIZE, verbose=1, validation_data=(X_valid, valid_labels))\n",
    "    score = model.evaluate(X_test, test_labels, batch_size=DEFAULT_BATCH_SIZE, verbose=1)\n",
    "    results.append({'Hyperparameter': 'LearningRate', \n",
    "                    'Value': learning_r, \n",
    "                    'Default_BatchSize': DEFAULT_BATCH_SIZE,\n",
    "                    'Default_NumHiddenLayers': DEFAULT_NUMBER_HIDDEN, \n",
    "                    'Default_LearningRate': DEFAULT_LEARNING_RATE, \n",
    "                    'Accuracy': score[1], \n",
    "                    'Loss': score[0]})\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "df.to_csv('hyperparameter_tuning_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loaded the tuning results into a DataFrame and picked out the batch sizes, hidden layer counts, accuracy, and loss. Made a grid over batch sizes and hidden layers and used it to guess accuracy and loss in between our actual data points. Started a plot to show this visually.\n",
    "\n",
    "Looking at the plots, it's clear that upping the batch size improves accuracy and decreases loss until it hits a plateau around the 20-40 mark. Past that, cranking up the batch size doesn't make much of a dent. As for hidden layers, there's a point of diminishing returns there too, once you have a good amount, adding extra doesn't seem to boost performance. And about learning rateâ€”there isn't a drastic change in accuracy with different rates, suggesting that once you nail the batch size and hidden layers, the learning rate can be fine-tuned for potentially better results, but it's not the main player.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'z' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m zi2_bn \u001b[38;5;241m=\u001b[39m griddata((x, y), z2, (xi_bn, yi_bn), method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcubic\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m yi_hl \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(y\u001b[38;5;241m.\u001b[39mmin(), y\u001b[38;5;241m.\u001b[39mmax(), \u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m zi_hl \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[43mz\u001b[49m\u001b[38;5;241m.\u001b[39mmin(), z\u001b[38;5;241m.\u001b[39mmax(), \u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m     17\u001b[0m yi_hl, zi_hl \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmeshgrid(yi_hl, zi_hl)\n\u001b[0;32m     19\u001b[0m zi1_hl \u001b[38;5;241m=\u001b[39m griddata((y, z), z1, (yi_hl, zi_hl), method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcubic\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'z' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('hyperparameter_tuning_results.csv')\n",
    "\n",
    "x = df['BatchSize']\n",
    "y = df['NumHiddenLayers']\n",
    "z = df['LearningRate']\n",
    "z1 = df['Accuracy']\n",
    "z2 = df['Loss']\n",
    "\n",
    "xi_bn = np.linspace(x.min(), x.max(), 100)\n",
    "yi_bn = np.linspace(y.min(), y.max(), 100)\n",
    "xi_bn, yi_bn = np.meshgrid(xi_bn, yi_bn)\n",
    "\n",
    "zi1_bn = griddata((x, y), z1, (xi_bn, yi_bn), method='cubic')\n",
    "zi2_bn = griddata((x, y), z2, (xi_bn, yi_bn), method='cubic')\n",
    "\n",
    "yi_hl = np.linspace(y.min(), y.max(), 100)\n",
    "zi_hl = np.linspace(z.min(), z.max(), 100)\n",
    "yi_hl, zi_hl = np.meshgrid(yi_hl, zi_hl)\n",
    "\n",
    "zi1_hl = griddata((y, z), z1, (yi_hl, zi_hl), method='cubic')\n",
    "zi2_hl = griddata((y, z), z2, (yi_hl, zi_hl), method='cubic')\n",
    "\n",
    "xi_lr = np.linspace(x.min(), x.max(), 100)\n",
    "zi_lr = np.linspace(z.min(), z.max(), 100)\n",
    "xi_lr, zi_lr = np.meshgrid(xi_lr, zi_lr)\n",
    "\n",
    "zi1_lr = griddata((x, z), z1, (xi_lr, zi_lr), method='cubic')\n",
    "zi2_lr = griddata((x, z), z2, (xi_lr, zi_lr), method='cubic')\n",
    "\n",
    "fig = plt.figure(figsize=(16, 24))\n",
    "\n",
    "# Plot for Accuracy vs. Batch Size & Number of Hidden Layers\n",
    "ax1 = fig.add_subplot(3, 2, 1, projection='3d')\n",
    "surf1 = ax1.plot_surface(xi_bn, yi_bn, zi1_bn, cmap='viridis', edgecolor='none')\n",
    "ax1.set_title('Accuracy vs. Batch Size & NumHiddenLayers')\n",
    "ax1.set_xlabel('Batch Size')\n",
    "ax1.set_ylabel('NumHiddenLayers')\n",
    "ax1.set_zlabel('Accuracy')\n",
    "\n",
    "# Plot for Loss vs. Batch Size & Number of Hidden Layers\n",
    "ax2 = fig.add_subplot(3, 2, 2, projection='3d')\n",
    "surf2 = ax2.plot_surface(xi_bn, yi_bn, zi2_bn, cmap='inferno', edgecolor='none')\n",
    "ax2.set_title('Loss vs. Batch Size & NumHiddenLayers')\n",
    "ax2.set_xlabel('Batch Size')\n",
    "ax2.set_ylabel('NumHiddenLayers')\n",
    "ax2.set_zlabel('Loss')\n",
    "\n",
    "# Plot for Accuracy vs. NumHiddenLayers & Learning Rate\n",
    "ax3 = fig.add_subplot(3, 2, 3, projection='3d')\n",
    "surf3 = ax3.plot_surface(yi_hl, zi_hl, zi1_hl, cmap='viridis', edgecolor='none')\n",
    "ax3.set_title('Accuracy vs. NumHiddenLayers & LearningRate')\n",
    "ax3.set_xlabel('NumHiddenLayers')\n",
    "ax3.set_ylabel('LearningRate')\n",
    "ax3.set_zlabel('Accuracy')\n",
    "\n",
    "# Plot for Loss vs. NumHiddenLayers & Learning Rate\n",
    "ax4 = fig.add_subplot(3, 2, 4, projection='3d')\n",
    "surf4 = ax4.plot_surface(yi_hl, zi_hl, zi2_hl, cmap='inferno', edgecolor='none')\n",
    "ax4.set_title('Loss vs. NumHiddenLayers & LearningRate')\n",
    "ax4.set_xlabel('NumHiddenLayers')\n",
    "ax4.set_ylabel('LearningRate')\n",
    "ax4.set_zlabel('Loss')\n",
    "\n",
    "# Plot for Accuracy vs. Batch Size & Learning Rate\n",
    "ax5 = fig.add_subplot(3, 2, 5, projection='3d')\n",
    "surf5 = ax5.plot_surface(xi_lr, zi_lr, zi1_lr, cmap='viridis', edgecolor='none')\n",
    "ax5.set_title('Accuracy vs. Batch Size & LearningRate')\n",
    "ax5.set_xlabel('Batch Size')\n",
    "ax5.set_ylabel('LearningRate')\n",
    "ax5.set_zlabel('Accuracy')\n",
    "\n",
    "# Plot for Loss vs. Batch Size & Learning Rate\n",
    "ax6 = fig.add_subplot(3, 2, 6, projection='3d')\n",
    "surf6 = ax6.plot_surface(xi_lr, zi_lr, zi2_lr, cmap='inferno', edgecolor='none')\n",
    "ax6.set_title('Loss vs. Batch Size & LearningRate')\n",
    "ax6.set_xlabel('Batch Size')\n",
    "ax6.set_ylabel('LearningRate')\n",
    "ax6.set_zlabel('Loss')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Size (batch-size.png)\n",
    "The graph shows a downward trend in validation accuracy as the batch size increases. This could be due to the fact that larger batch sizes provide a more accurate estimate of the gradient, but smaller batches may offer a regularizing effect and more frequent updates, which can lead to better generalization. A smaller batch size also means the model is updated more frequently, which can lead to faster learning but also increased noise in the gradient estimation.\n",
    "\n",
    "**Hypothesis**: Smaller batch sizes may be forcing the model to learn more robust features due to noise in the gradient estimates, leading to better generalization on the validation set.\n",
    "\n",
    "## Learning Rate (learning-rate.png)\n",
    "In this graph, the validation accuracy improves significantly as the learning rate increases from 0.01 to 0.05, then stabilizes and starts to decrease after 0.1. This suggests that a learning rate that is too low may not traverse the loss landscape efficiently, while a learning rate that is too high might overshoot optimal points.\n",
    "\n",
    "**Hypothesis**: There's an optimal range of learning rates where the model learns efficiently without overshooting or getting stuck in local minima. Beyond this range, the learning either becomes inefficient (too low) or too erratic (too high).\n",
    "\n",
    "## Number of Hidden Layers (num-hidden-layers.png)\n",
    "The trend in this graph is less straightforward, showing a peak in accuracy at 4 hidden layers, followed by a dip at 6 layers, and then an increase at 8 layers. This could indicate that the model complexity appropriate for the given task is achieved at around 4 hidden layers, but further increases may lead to overfitting, which is possibly mitigated when the number of layers is increased to 8.\n",
    "\n",
    "**Hypothesis**: The model complexity needed to capture the patterns in the data is met at a certain number of layers, beyond which the model might overfit. The subsequent increase in accuracy with more layers might be due to the model's ability to capture more abstract representations.\n",
    "\n",
    "**General Considerations**\n",
    "Accuracy should not be the only criterion for deciding a parameter setting. Other considerations include:\n",
    "\n",
    "**Overfitting**: High accuracy on the validation set might not translate to the test set if the model has overfitted.\n",
    "Training Time: Larger batch sizes and more hidden layers generally increase training time.\n",
    "**Computational Resources**: More complex models with more layers and smaller batch sizes might require more memory and computational power.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
